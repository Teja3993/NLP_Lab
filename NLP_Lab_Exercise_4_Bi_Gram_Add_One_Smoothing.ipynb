{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJoM2nTMS5QvkJkzQEYbvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teja3993/NLP_Lab/blob/main/NLP_Lab_Exercise_4_Bi_Gram_Add_One_Smoothing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW8EdalZVpkl",
        "outputId": "d1af4b8e-d12a-4e31-f9ab-e97cd45640b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
            "\n",
            "Unigram Frequencies: {'This': 3, 'is': 3, 'a': 2, 'dog': 1, 'cat': 2, 'I': 1, 'love': 1, 'my': 2}\n",
            "Bigram Frequencies: {('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
            "\n",
            "--- Raw Probabilities (Matches PDF Output) ---\n",
            "('This', 'is'): 1.0000\n",
            "('is', 'a'): 0.6667\n",
            "('a', 'dog'): 0.5000\n",
            "('a', 'cat'): 0.5000\n",
            "('I', 'love'): 1.0000\n",
            "('love', 'my'): 1.0000\n",
            "('my', 'cat'): 0.5000\n",
            "('is', 'my'): 0.3333\n",
            "('my', 'name'): 0.5000\n",
            "\n",
            "--- Testing Sentence: \"This is my cat\" ---\n",
            "P(is|This) = 1.0\n",
            "P(my|is) = 0.3333333333333333\n",
            "P(cat|my) = 0.5\n",
            "Total Probability: 0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "# --- Experiment 4: Bigram Model & Add-One Smoothing ---\n",
        "\n",
        "def get_data():\n",
        "    # The dataset from the PDF\n",
        "    data = ['This is a dog', 'This is a cat', 'I love my cat', 'This is my name']\n",
        "\n",
        "    # Flatten into a single list of words\n",
        "    tokens = []\n",
        "    for sentence in data:\n",
        "        for word in sentence.split():\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "def build_models(tokens):\n",
        "    # Dictionaries to store counts\n",
        "    bigram_counts = {}\n",
        "    unigram_counts = {}\n",
        "    bigram_list = []\n",
        "\n",
        "    # Vocabulary size (V) for smoothing\n",
        "    unique_words = set(tokens)\n",
        "    V = len(unique_words)\n",
        "\n",
        "    # Logic to create bigrams\n",
        "    # The PDF logic skips bigrams if the second word is Capitalized\n",
        "    # (assuming it's the start of a new sentence)\n",
        "    for i in range(len(tokens) - 1):\n",
        "        current_word = tokens[i]\n",
        "        next_word = tokens[i+1]\n",
        "\n",
        "        # Update Unigram Count\n",
        "        if current_word in unigram_counts:\n",
        "            unigram_counts[current_word] += 1\n",
        "        else:\n",
        "            unigram_counts[current_word] = 1\n",
        "\n",
        "        # Check condition to form bigram (heuristics from PDF)\n",
        "        # We only form a bigram if the next word is lowercase (part of same sentence)\n",
        "        if next_word.islower():\n",
        "            bigram = (current_word, next_word)\n",
        "            bigram_list.append(bigram)\n",
        "\n",
        "            if bigram in bigram_counts:\n",
        "                bigram_counts[bigram] += 1\n",
        "            else:\n",
        "                bigram_counts[bigram] = 1\n",
        "\n",
        "    return bigram_list, unigram_counts, bigram_counts, V\n",
        "\n",
        "def calculate_probabilities(bigram_counts, unigram_counts, V):\n",
        "    raw_probs = {}\n",
        "    smoothed_probs = {}\n",
        "\n",
        "    # Iterate through all found bigrams\n",
        "    for bigram in bigram_counts:\n",
        "        word1 = bigram[0]\n",
        "        word2 = bigram[1]\n",
        "\n",
        "        count_w1w2 = bigram_counts[bigram]\n",
        "        count_w1 = unigram_counts[word1]\n",
        "\n",
        "        # 1. Raw Probability Formula (Matches PDF Output)\n",
        "        raw_probs[bigram] = count_w1w2 / count_w1\n",
        "\n",
        "        # 2. Add-One Smoothing Formula (Matches PDF Aim)\n",
        "        # (Count + 1) / (UnigramCount + VocabularySize)\n",
        "        smoothed_probs[bigram] = (count_w1w2 + 1) / (count_w1 + V)\n",
        "\n",
        "    return raw_probs, smoothed_probs\n",
        "\n",
        "def test_sentence(sentence, raw_probs):\n",
        "    print(f\"\\n--- Testing Sentence: \\\"{sentence}\\\" ---\")\n",
        "    words = sentence.split()\n",
        "    total_prob = 1.0\n",
        "    found_bigrams = []\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        bg = (words[i], words[i+1])\n",
        "        found_bigrams.append(bg)\n",
        "\n",
        "        if bg in raw_probs:\n",
        "            prob = raw_probs[bg]\n",
        "            print(f\"P({bg[1]}|{bg[0]}) = {prob}\")\n",
        "            total_prob *= prob\n",
        "        else:\n",
        "            print(f\"P({bg[1]}|{bg[0]}) = 0.0 (Not found)\")\n",
        "            total_prob *= 0.0\n",
        "\n",
        "    print(f\"Total Probability: {total_prob}\")\n",
        "\n",
        "# --- Execution ---\n",
        "tokens = get_data()\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Build Counts\n",
        "bigram_list, unigram_counts, bigram_counts, V = build_models(tokens)\n",
        "\n",
        "# Calculate Probs\n",
        "raw_probs, smoothed_probs = calculate_probabilities(bigram_counts, unigram_counts, V)\n",
        "\n",
        "# Display Results (Similar to PDF Output)\n",
        "print(\"\\nUnigram Frequencies:\", unigram_counts)\n",
        "print(\"Bigram Frequencies:\", bigram_counts)\n",
        "\n",
        "print(\"\\n--- Raw Probabilities (Matches PDF Output) ---\")\n",
        "for bg, prob in raw_probs.items():\n",
        "    print(f\"{bg}: {prob:.4f}\")\n",
        "\n",
        "# Run the Test Case from the PDF\n",
        "test_sentence(\"This is my cat\", raw_probs)"
      ]
    }
  ]
}